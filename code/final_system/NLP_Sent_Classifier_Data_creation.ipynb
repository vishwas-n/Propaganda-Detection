{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AVFv0g_khYo_"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oY9vUWYqhmDb",
    "outputId": "b29f786d-0de5-428a-bcf7-b6029710654d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yu-RabBHhnWn"
   },
   "outputs": [],
   "source": [
    "train_folder = \"/content/drive/MyDrive/NLP/project_5_data/datasets/train-articles\" \n",
    "dev_folder = \"/content/drive/MyDrive/NLP/project_5_data/datasets/dev-articles\"    \n",
    "train_labels_file = \"/content/drive/MyDrive/NLP/project_5_data/datasets/train-labels-task-si/\"\n",
    "dev_labels_file = \"/content/drive/MyDrive/NLP/project_5_data/datasets/dev-labels-task-si\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zA9oN5L6hovi"
   },
   "outputs": [],
   "source": [
    "def read_articles_from_file_list(folder_name, file_pattern=\"*.txt\"):\n",
    "    \"\"\"\n",
    "    Read articles from files matching patterns <file_pattern> from  \n",
    "    the directory <folder_name>. \n",
    "    The content of the article is saved in the dictionary whose key\n",
    "    is the id of the article (extracted from the file name).\n",
    "    Each element of <sentence_list> is one line of the article.\n",
    "    \"\"\"\n",
    "    file_list = glob.glob(os.path.join(folder_name, file_pattern))\n",
    "    articles = {}\n",
    "    article_id_list, sentence_id_list, sentence_list = ([], [], [])\n",
    "    for filename in sorted(file_list):\n",
    "        article_id = os.path.basename(filename).split(\".\")[0][7:]\n",
    "        with codecs.open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "            articles[article_id] = f.read()\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Bg0jS7l3eZLB"
   },
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join(train_folder, \"*.txt\"))\n",
    "train_articles_content, train_articles_id = ([], [])\n",
    "for filename in file_list:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        train_articles_content.append(' '.join([line.strip() for line in f]))\n",
    "        train_articles_id.append(os.path.basename(filename).split(\".\")[0][7:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t82UPVQizKnx",
    "outputId": "29411716-dd79-4b3a-ff22-26966ec331d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(371, 75)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_articles = read_articles_from_file_list(train_folder)\n",
    "dev_articles = read_articles_from_file_list(dev_folder)\n",
    "\n",
    "len(train_articles), len(dev_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jCD14ScF0DRa"
   },
   "outputs": [],
   "source": [
    "TASK_3_ARTICLE_ID_COL = 0\n",
    "#TASK_3_TECHNIQUE_NAME_COL = 1\n",
    "TASK_3_FRAGMENT_START_COL = 1\n",
    "TASK_3_FRAGMENT_END_COL = 2\n",
    "\n",
    "def extract_article_id_from_file_name(fullpathfilename):\n",
    "\n",
    "    regex = re.compile(\"article([0-9]+).*\")\n",
    "    return regex.match(os.path.basename(fullpathfilename)).group(1)\n",
    "\n",
    "   \n",
    "def load_annotation_list_from_folder(folder_name, techniques_names):\n",
    "\n",
    "    file_list = glob.glob(os.path.join(folder_name, \"*.labels\"))\n",
    "    if len(file_list)==0:\n",
    "        print(\"Cannot load file list in folder \" + folder_name)\n",
    "        sys.exit()\n",
    "    annotations = {}\n",
    "    for filename in file_list:\n",
    "        annotations[extract_article_id_from_file_name(filename)] = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            for row_number, line in enumerate(f.readlines()):\n",
    "                row = line.rstrip().split(\"\\t\")\n",
    "                annotations[row[TASK_3_ARTICLE_ID_COL]].append((row[TASK_3_FRAGMENT_START_COL], row[TASK_3_FRAGMENT_END_COL]))\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Oj4NcyRg2TMR"
   },
   "outputs": [],
   "source": [
    "techniques_names = [ \"propaganda\" ]\n",
    "train_annotation = load_annotation_list_from_folder(train_labels_file, techniques_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bTc14E5gHXOG"
   },
   "outputs": [],
   "source": [
    "train_labels = {}\n",
    "for article in train_articles.keys():\n",
    "    labels = [0] * len(train_articles[article])\n",
    "    for annot in train_annotation[article]:\n",
    "        labels[int(annot[0]):int(annot[1])+1] = [1] * (int(annot[1]) - int(annot[0]) + 1)\n",
    "    train_labels[article] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OS8smk0JVp_V",
    "outputId": "01ce82ae-6f28-404d-bc56-d596fcd537e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda sentences: 4832.  Total sentences: 16690 \n"
     ]
    }
   ],
   "source": [
    "train_labels_str = {}\n",
    "with open(\"/content/drive/MyDrive/NLP/train_sentence_classification.txt\", \"w\") as fout:\n",
    "        \n",
    "    count = 0\n",
    "    sentence_count = 0\n",
    "    for article_id in train_articles.keys():\n",
    "\n",
    "        index = 0\n",
    "        word_index = 0\n",
    "        # labels = [0] * len(train_articles[article_id].replace('\\n\\n',' ').split(' '))\n",
    "        labels = [0] * len(train_articles[article_id].replace('\\n\\n',' ').replace('\\n', ' ').strip().split(' '))\n",
    "        labels_str = ['O'] * len(labels)\n",
    "\n",
    "\n",
    "        for sentence in train_articles[article_id].replace('\\n\\n', '\\n').strip().split('\\n'):\n",
    "            sentence_is_prop = False\n",
    "            sentence_count += 1\n",
    "            for word in sentence.split(' '):\n",
    "                if train_labels[article_id][index] == 1:\n",
    "                    labels[word_index] = 1\n",
    "                    labels_str[word_index] = 'I-Prop'\n",
    "                    sentence_is_prop = True\n",
    "                word_index += 1\n",
    "                index += len(word) + 1\n",
    "            # index += 1\n",
    "            if sentence_is_prop:\n",
    "                count += 1\n",
    "                fout.write(\"%s\\t%s\\t%s\\n\" % (article_id, sentence, 1))\n",
    "            else:\n",
    "                fout.write(\"%s\\t%s\\t%s\\n\" % (article_id, sentence, 0))\n",
    "        train_labels[article_id] = labels\n",
    "        train_labels_str[article_id] = labels_str\n",
    "print(\"Propaganda sentences: {}.  Total sentences: {} \".format(count, sentence_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vxrPb2uvi9yA"
   },
   "outputs": [],
   "source": [
    "dev_annotation = load_annotation_list_from_folder(dev_labels_file, techniques_names)\n",
    "\n",
    "dev_labels = {}\n",
    "for article in dev_articles.keys():\n",
    "    labels = [0] * len(dev_articles[article])\n",
    "    for annot in dev_annotation[article]:\n",
    "        labels[int(annot[0]):int(annot[1])] = [1] * (int(annot[1]) - int(annot[0]))\n",
    "    dev_labels[article] = labels\n",
    "\n",
    "\n",
    "dev_labels_str = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWyVubSXi_c_",
    "outputId": "994c50f8-65be-43ab-90eb-5d58df070a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propaganda sentences: 787.  Total sentences: 3177 \n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/NLP/dev_sentence_classification.txt\", \"w\") as fout:\n",
    "\n",
    "    count = 0\n",
    "    sentence_count = 0\n",
    "    for article_id in dev_articles.keys():\n",
    "        index = 0\n",
    "        word_index = 0\n",
    "\n",
    "        labels = [0] * len(dev_articles[article_id].replace('\\n\\n','\\n').replace('\\n', ' ').strip().split(' '))\n",
    "        labels_str = ['O'] * len(labels)\n",
    "        \n",
    "        first_sentence = True\n",
    "        for sentence in dev_articles[article_id].replace('\\n\\n', '\\n').strip().split('\\n'):\n",
    "            sentence_is_prop = False\n",
    "            sentence_count += 1\n",
    "            for word in sentence.split(' '):\n",
    "                if dev_labels[article_id][index] == 1:\n",
    "                    labels[word_index] = 1\n",
    "                    labels_str[word_index] = 'I-Prop'\n",
    "                    sentence_is_prop = True\n",
    "                word_index += 1\n",
    "                index += len(word) + 1\n",
    "            if first_sentence:\n",
    "                first_sentence = False\n",
    "                index += 1\n",
    "            if sentence_is_prop:\n",
    "                count += 1\n",
    "                fout.write(\"%s\\t%s\\t%s\\n\" % (article_id, sentence, 1))\n",
    "            else:\n",
    "                fout.write(\"%s\\t%s\\t%s\\n\" % (article_id, sentence, 0))\n",
    "            # index += 1\n",
    "\n",
    "        dev_labels[article_id] = labels\n",
    "        dev_labels_str[article_id] = labels_str\n",
    "print(\"Propaganda sentences: {}.  Total sentences: {} \".format(count, sentence_count))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP_SC_Dataset_creation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
