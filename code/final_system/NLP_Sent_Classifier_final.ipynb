{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYb4b2dbXqY7"
   },
   "outputs": [],
   "source": [
    "#  !pip install transformers seqeval[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6yIqY1uXUyl"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WkJISARMfdS"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaTokenizerFast, RobertaConfig, RobertaForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import cuda\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sys\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMJAleGYNglf",
    "outputId": "25a7a709-6e11-4b86-e2c9-1a2b1e340762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9rBtiyiXq3p",
    "outputId": "8bb49f0a-e70e-48ff-96aa-4a1a7ecd80aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3Zlj6HWpuN8"
   },
   "outputs": [],
   "source": [
    "#Read Sentences from the stored files\n",
    "def read_sentences_and_labels(filename):\n",
    "    sentences, labels = [], []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            article_id, sentence, label = line.strip().split('\\t')\n",
    "            sentences.append(sentence.lower())\n",
    "            labels.append(int(label))\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YU4Zg6aieW_O"
   },
   "outputs": [],
   "source": [
    "#Load Training and Dev Data\n",
    "\n",
    "train_sentences, train_labels = read_sentences_and_labels(\"/content/drive/MyDrive/NLP/train_sentence_classification.txt\")\n",
    "dev_sentences, dev_labels = read_sentences_and_labels(\"/content/drive/MyDrive/NLP/dev_sentence_classification.txt\")\n",
    "\n",
    "print(\"Train Size:\", len(train_sentences), len(train_labels))\n",
    "print(\"Dev Size:\", len(dev_sentences), len(dev_labels))\n",
    "print(\"Train Data Propanda Sentences:\", sum(train_labels))\n",
    "print(\"Dev Data Propanda Sentences:\", sum(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSwqJ4VJeCLz"
   },
   "outputs": [],
   "source": [
    "# Sample the Training data to overcome data imbalance\n",
    "# Randomize and Pick 5000 Train examples\n",
    "\n",
    "train_labels_np = np.array(train_labels)\n",
    "train_sentences_np = np.array(train_sentences)\n",
    "\n",
    "indices_of_1 = np.where(train_labels_np == 1)[0]\n",
    "indices_of_0 = np.where(train_labels_np == 0)[0]\n",
    "\n",
    "subset_of_0 = np.random.choice(indices_of_0, size=5000, replace=False)\n",
    "\n",
    "final_indices = np.append(subset_of_0, indices_of_1)\n",
    "np.random.shuffle(final_indices)\n",
    "\n",
    "train_sentences_final = train_sentences_np[final_indices]\n",
    "train_labels_final = train_labels_np[final_indices]\n",
    "\n",
    "idx = train_sentences.index(train_sentences_final[0])\n",
    "assert train_labels_final[0] == train_labels[idx]\n",
    "\n",
    "idx = train_sentences.index(train_sentences_final[50])\n",
    "assert train_labels_final[50] == train_labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2Ulnyhjyew3",
    "outputId": "17ef06d2-ff50-456c-b3bb-396a3ae3de8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['credit for this dangerous scenario — in which getting an ar-15-style rifle is just a matter of a few computer clicks — goes to the trump administration for its inexplicable decision to settle a lawsuit it was on the verge of winning.',\n",
       "       'and if so, could we have been this wrong?',\n",
       "       '\"the dna of our culture is preserved.', ...,\n",
       "       'the duran’s alex christoforou and editor-in-chief alexander mercouris discuss the insurrection taking place at the us department of justice, as democrats, ex-obama officials, and doj directors are doing everything in their power to make sure the truth, about how the fisa warrant to spy on carter page was obtained, remains hidden from the eyes of the american public.',\n",
       "       '8 – in coming to goldman sachs, powell joined a firm that has long been deeply tied to the clintons.',\n",
       "       'the inspector general made clear when he launched his investigation in january 2017 that “his review will not substitute the oig\\'s judgment for the judgments made by the fbi or the department regarding the substantive merits of investigative or prosecutive decisions.\"'],\n",
       "      dtype='<U817')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMmWDGiioDxk"
   },
   "outputs": [],
   "source": [
    "# # Code to perform lemmatization or Stemming\n",
    "# # Commenting out entire cell as either approach didn't yield higher accuracy. \n",
    "# # Rather these approaches reduced the accuracy\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "# stemmer = nltk.stem.SnowballStemmer('english')\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# train_sentences_stemmed = []\n",
    "# for sent in train_sentences_final:\n",
    "#     # words = nltk.word_tokenize(sent)\n",
    "#     words = sent.split(' ')\n",
    "#     stemmed_output = ' '.join([lemmatizer.lemmatize(w) for w in words if len(w) > 1])\n",
    "#     train_sentences_stemmed.append(stemmed_output)\n",
    "\n",
    "\n",
    "# dev_sentences_stemmed = []\n",
    "# for sent in dev_sentences:\n",
    "#     words = nltk.word_tokenize(sent)\n",
    "#     stemmed_output = ' '.join([lemmatizer.lemmatize(w) for w in words if len(w) > 1])\n",
    "#     dev_sentences_stemmed.append(stemmed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtumDY7Qo7Zx"
   },
   "outputs": [],
   "source": [
    "# print(train_sentences_final[0:3])\n",
    "# print(train_sentences_stemmed[0:3])\n",
    "# len(train_sentences_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVwm1yUPM62z"
   },
   "outputs": [],
   "source": [
    "#Torch Dataset definition\n",
    "class SC_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVU2nHmnisMO"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "DEV_BATCH_SIZE = 4\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_GRAD_NORM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJ4ePHvFPP9c"
   },
   "outputs": [],
   "source": [
    "#Load pretrained tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(set(train_labels)))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClBPxxMYMhrV"
   },
   "outputs": [],
   "source": [
    "print(tokenizer(train_sentences_final[0]))\n",
    "\n",
    "train_encodings = tokenizer(train_sentences_final.tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n",
    "dev_encodings = tokenizer(dev_sentences, truncation=True, padding=True, max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNkTDbDcNm3s"
   },
   "outputs": [],
   "source": [
    "train_dataset = SC_Dataset(train_encodings, train_labels_final)\n",
    "dev_dataset = SC_Dataset(dev_encodings, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Usg6tiN-U-Sc"
   },
   "outputs": [],
   "source": [
    "len(train_dataset), len(dev_dataset), train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EifAo7mAMogL"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=DEV_BATCH_SIZE, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHwBAE_6QL7Q"
   },
   "outputs": [],
   "source": [
    "#Code to train the model with our dataset\n",
    "def train(epoch):\n",
    "\n",
    "    model.train()\n",
    "    tr_loss, nb_tr_steps  = 0, 0\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        if (idx+1) % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "\n",
    "        # # gradient clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(\n",
    "        #     parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        # )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # # Uncomment to save model after each epoch \n",
    "    # torch.save(model, \"/content/drive/MyDrive/NLP/RoBERTa_Task_SentClassify.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlmSrG7USyzE"
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)\n",
    "    print(\"Model saved after training for {} epochs\".format(epoch+1))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lu2jklep7Dds"
   },
   "outputs": [],
   "source": [
    "# Save and Load the model \n",
    "# Comment/Uncomment as required\n",
    "torch.save(model, \"/content/drive/MyDrive/NLP/RoBERTa_Task_SentClassify.pt.pt\")\n",
    "# model = torch.load(\"/content/drive/MyDrive/NLP/RoBERTa_Task_SentClassify.pt\", map_location=torch.device('cpu'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lIXVACeXGjC"
   },
   "outputs": [],
   "source": [
    "#Get predictions for Dev dataset\n",
    "import numpy as np\n",
    "model.eval()\n",
    "result = 0\n",
    "predictions,  true_labels = [], []\n",
    "\n",
    "bat_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "        bat_test = batch\n",
    "        ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        result = model(ids, attention_mask=mask, labels=labels)\n",
    "        logits = result[1]\n",
    "\n",
    "        predictions.append(logits.detach().cpu().numpy())\n",
    "        true_labels.append(labels.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvRUEhSpZT2u"
   },
   "outputs": [],
   "source": [
    "true_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isnWFH59brei"
   },
   "outputs": [],
   "source": [
    "flat_predictions = np.concatenate(predictions, axis = 0)\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = np.concatenate(true_labels, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RV_c4tUb-29"
   },
   "outputs": [],
   "source": [
    "#Compute the model metrics\n",
    "print(\"Dev Accuracy:\", accuracy_score(flat_true_labels, flat_predictions))\n",
    "print(\"Dev Precision:\", precision_score(flat_true_labels, flat_predictions))\n",
    "print(\"Dev Recall:\", recall_score(flat_true_labels, flat_predictions))\n",
    "print(\"Dev F1 score:\", f1_score(flat_true_labels, flat_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dcETQ5yuP7H"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_Phase3_SI_preprocess_SentenceClassification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
